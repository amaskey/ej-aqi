{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RJCCOqVpQo_"
      },
      "outputs": [],
      "source": [
        "# Version 1\n",
        "# CNN-LSTM implementation to predict air quality index\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gN2xwIocpQpB"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('data/ml-ready/daily-AQI-socio-economic-ml-ready.csv')\n",
        "print(df)\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okzMEXaopQpC"
      },
      "outputs": [],
      "source": [
        "df.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdxjNS8DpQpC"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ViqoDSGwpQpC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Preprocess the data\n",
        "features = df.drop(columns=['Date Local','AQI']).values\n",
        "\n",
        "target = df['AQI'].values\n",
        "\n",
        "# Normalize the features\n",
        "scaler = StandardScaler()\n",
        "features_scaled = scaler.fit_transform(features)\n",
        "\n",
        "# Create sequences for LSTM\n",
        "def create_sequences(data, target, sequence_length=5):\n",
        "    sequences = []\n",
        "    labels = []\n",
        "    for i in range(len(data) - sequence_length):\n",
        "        sequences.append(data[i:i + sequence_length])\n",
        "        labels.append(target[i + sequence_length])\n",
        "    return np.array(sequences), np.array(labels)\n",
        "\n",
        "sequence_length = 15\n",
        "X, y = create_sequences(features_scaled, target, sequence_length)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X = torch.tensor(X, dtype=torch.float32)\n",
        "y = torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create DataLoader for batching\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Define the CNN-LSTM model\n",
        "class CNN_LSTM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN_LSTM, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=X_train.shape[2], out_channels=64, kernel_size=2)\n",
        "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=2)\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
        "        self.lstm1 = nn.LSTM(input_size=128, hidden_size=100, batch_first=True)\n",
        "        self.lstm2 = nn.LSTM(input_size=100, hidden_size=50, batch_first=True)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(50, 25)\n",
        "        self.fc2 = nn.Linear(25, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)  # Change shape to (batch_size, num_features, sequence_length)\n",
        "        x = self.conv1(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.pool(x)\n",
        "        x = x.permute(0, 2, 1)  # Change shape back to (batch_size, sequence_length, num_features)\n",
        "        x, (hn, cn) = self.lstm1(x)\n",
        "        x, (hn, cn) = self.lstm2(x)\n",
        "        x = self.dropout(x[:, -1, :])\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = CNN_LSTM()\n",
        "\n",
        "# Define loss and optimizer\n",
        "criterion = nn.MSELoss() #1/n(y_hat - y)squared\n",
        "# criterion = nn.SmoothL1Loss()  # Huber loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 250\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_X)\n",
        "        loss = criterion(outputs.squeeze(), batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # train_loss /= len(train_loader)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_X, batch_y in test_loader:\n",
        "            outputs = model(batch_X)\n",
        "            loss = criterion(outputs.squeeze(), batch_y)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    # val_loss /= len(test_loader)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(test_loader):.4f}')\n",
        "\n",
        "# Evaluate the model\n",
        "model.eval()\n",
        "test_loss = 0\n",
        "with torch.no_grad():\n",
        "    for batch_X, batch_y in test_loader:\n",
        "        outputs = model(batch_X)\n",
        "        loss = criterion(outputs.squeeze(), batch_y)\n",
        "        test_loss += loss.item()\n",
        "print(f'Test Loss: {test_loss/len(test_loader):.4f}')\n",
        "\n",
        "# Predict future AQI values\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    predictions = model(X_test).squeeze().numpy()\n",
        "print(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YUNPEFhipQpD"
      },
      "outputs": [],
      "source": [
        "print(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0a5sTYupQpD"
      },
      "outputs": [],
      "source": [
        "print(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0592Te5wpQpE"
      },
      "outputs": [],
      "source": [
        "# Save the trained model\n",
        "torch.save(model.state_dict(), 'deeper_cnn_lstm_attention_model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m491OkPBpQpE"
      },
      "outputs": [],
      "source": [
        "train_losses_scaled = [x/len(train_loader) for x in train_losses]\n",
        "val_losses_scaled = [x/len(train_loader) for x in val_losses]\n",
        "\n",
        "# Plot the training and validation loss\n",
        "epochs = range(1, len(train_losses) + 1)\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(epochs, train_losses_scaled, label='Train Loss')\n",
        "plt.plot(epochs, val_losses_scaled, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-14mM-1pQpE"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "true_values = np.array(y_test)\n",
        "\n",
        "# Calculate performance metrics\n",
        "mse = mean_squared_error(true_values, predictions)\n",
        "rmse = np.sqrt(mse)\n",
        "mae = mean_absolute_error(true_values, predictions)\n",
        "r2 = r2_score(true_values, predictions)\n",
        "\n",
        "# Define a tolerance level for custom accuracy\n",
        "tolerance = 0.3  # 30% tolerance\n",
        "accuracy = np.mean(np.abs(predictions - true_values) / true_values < tolerance) * 100\n",
        "\n",
        "print(f'MSE: {mse:.4f}')\n",
        "print(f'RMSE: {rmse:.4f}')\n",
        "print(f'MAE: {mae:.4f}')\n",
        "print(f'RÂ²: {r2:.4f}')\n",
        "print(f'Accuracy: {accuracy:.2f}%')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}